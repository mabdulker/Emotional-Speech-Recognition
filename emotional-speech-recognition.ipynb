{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-19T13:38:09.589610Z","iopub.execute_input":"2022-07-19T13:38:09.590341Z","iopub.status.idle":"2022-07-19T13:38:21.659278Z","shell.execute_reply.started":"2022-07-19T13:38:09.590166Z","shell.execute_reply":"2022-07-19T13:38:21.657826Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Imports**","metadata":{}},{"cell_type":"code","source":"# Importing required libraries \n# Keras\nfrom tensorflow import keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other  \nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nimport pickle\nimport IPython.display as ipd  # To play sound in the notebook\nimport sys\nimport warnings","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:38:21.661637Z","iopub.execute_input":"2022-07-19T13:38:21.662145Z","iopub.status.idle":"2022-07-19T13:38:36.346103Z","shell.execute_reply.started":"2022-07-19T13:38:21.662106Z","shell.execute_reply":"2022-07-19T13:38:36.344509Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:38:36.348469Z","iopub.execute_input":"2022-07-19T13:38:36.349522Z","iopub.status.idle":"2022-07-19T13:38:36.356609Z","shell.execute_reply.started":"2022-07-19T13:38:36.349478Z","shell.execute_reply":"2022-07-19T13:38:36.354942Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Beginning of model","metadata":{}},{"cell_type":"code","source":"CREMA = \"../input/speech-emotion-recognition-en/Crema/\"\n\ndir_list = os.listdir(CREMA)\ndir_list.sort()\ndir_list","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:38:36.359073Z","iopub.execute_input":"2022-07-19T13:38:36.360198Z","iopub.status.idle":"2022-07-19T13:38:36.422176Z","shell.execute_reply.started":"2022-07-19T13:38:36.360055Z","shell.execute_reply":"2022-07-19T13:38:36.421048Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"gender = []\nemotion = []\npath = []\nfemale = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n\nfor i in dir_list: \n    part = i.split('_')\n    if int(part[0]) in female:\n        temp = 'female'\n    else:\n        temp = 'male'\n    gender.append(temp)\n    if part[2] == 'SAD' and temp == 'male':\n        emotion.append('male_sad')\n    elif part[2] == 'ANG' and temp == 'male':\n        emotion.append('male_angry')\n    elif part[2] == 'DIS' and temp == 'male':\n        emotion.append('male_disgust')\n    elif part[2] == 'FEA' and temp == 'male':\n        emotion.append('male_fear')\n    elif part[2] == 'HAP' and temp == 'male':\n        emotion.append('male_happy')\n    elif part[2] == 'NEU' and temp == 'male':\n        emotion.append('male_neutral')\n    elif part[2] == 'SAD' and temp == 'female':\n        emotion.append('female_sad')\n    elif part[2] == 'ANG' and temp == 'female':\n        emotion.append('female_angry')\n    elif part[2] == 'DIS' and temp == 'female':\n        emotion.append('female_disgust')\n    elif part[2] == 'FEA' and temp == 'female':\n        emotion.append('female_fear')\n    elif part[2] == 'HAP' and temp == 'female':\n        emotion.append('female_happy')\n    elif part[2] == 'NEU' and temp == 'female':\n        emotion.append('female_neutral')\n    else:\n        emotion.append('Unknown')\n    path.append(CREMA + i)\n    \nCREMA_df = pd.DataFrame(emotion, columns = ['labels'])\nCREMA_df['source'] = 'CREMA'\nCREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nCREMA_df.labels.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:38:36.424999Z","iopub.execute_input":"2022-07-19T13:38:36.425674Z","iopub.status.idle":"2022-07-19T13:38:36.495582Z","shell.execute_reply.started":"2022-07-19T13:38:36.425630Z","shell.execute_reply":"2022-07-19T13:38:36.494575Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# use the well known Librosa library for this task \nfname = CREMA + '1012_IEO_HAP_HI.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveshow(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:38:36.496844Z","iopub.execute_input":"2022-07-19T13:38:36.497379Z","iopub.status.idle":"2022-07-19T13:38:38.183003Z","shell.execute_reply.started":"2022-07-19T13:38:36.497346Z","shell.execute_reply":"2022-07-19T13:38:38.181499Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(CREMA_df.labels.value_counts())\nCREMA_df.head()\nCREMA_df.to_csv(\"crema.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:38:38.184467Z","iopub.execute_input":"2022-07-19T13:38:38.184823Z","iopub.status.idle":"2022-07-19T13:38:38.238678Z","shell.execute_reply.started":"2022-07-19T13:38:38.184790Z","shell.execute_reply":"2022-07-19T13:38:38.236981Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# lets pick up the meta-data that we got from our first part of the Kernel\nref = pd.read_csv(\"./crema.csv\")\nref.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:38:38.240436Z","iopub.execute_input":"2022-07-19T13:38:38.241112Z","iopub.status.idle":"2022-07-19T13:38:38.277927Z","shell.execute_reply.started":"2022-07-19T13:38:38.241074Z","shell.execute_reply":"2022-07-19T13:38:38.276797Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# # Note this takes a couple of minutes (~10 mins) as we're iterating over 4 datasets \ndf = pd.DataFrame(columns=['feature'])\n\n# loop feature extraction over the entire dataset\ncounter=0\nfor index,path in enumerate(ref.path):\n    X, sample_rate = librosa.load(path\n                                  , res_type='kaiser_fast'\n                                  ,duration=2.5\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n    sample_rate = np.array(sample_rate)\n    \n    # mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=sample_rate, \n                                        n_mfcc=13),\n                    axis=0)\n    df.loc[counter] = [mfccs]\n    counter=counter+1   \n\n# Check a few records to make sure its processed successfully\nprint(len(df))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:38:38.279815Z","iopub.execute_input":"2022-07-19T13:38:38.280553Z","iopub.status.idle":"2022-07-19T13:48:28.676196Z","shell.execute_reply.started":"2022-07-19T13:38:38.280510Z","shell.execute_reply":"2022-07-19T13:48:28.674911Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf[:5]","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:48:28.678122Z","iopub.execute_input":"2022-07-19T13:48:28.678851Z","iopub.status.idle":"2022-07-19T13:48:29.695504Z","shell.execute_reply.started":"2022-07-19T13:48:28.678805Z","shell.execute_reply":"2022-07-19T13:48:29.693864Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# replace NA with 0\ndf=df.fillna(0)\n# df=df.dropna()\nprint(df.shape)\ndf[:5]","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:48:29.697774Z","iopub.execute_input":"2022-07-19T13:48:29.698155Z","iopub.status.idle":"2022-07-19T13:48:29.744796Z","shell.execute_reply.started":"2022-07-19T13:48:29.698122Z","shell.execute_reply":"2022-07-19T13:48:29.743402Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n                                                    , df.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# Lets see how the data present itself before normalisation \nX_train[150:160]","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:48:29.746291Z","iopub.execute_input":"2022-07-19T13:48:29.746647Z","iopub.status.idle":"2022-07-19T13:48:29.800581Z","shell.execute_reply.started":"2022-07-19T13:48:29.746616Z","shell.execute_reply":"2022-07-19T13:48:29.799321Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Lts do data normalization \nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)/std\nX_test = (X_test - mean)/std\n\n# Check the dataset now \nX_train[150:160]","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:48:29.802163Z","iopub.execute_input":"2022-07-19T13:48:29.802572Z","iopub.status.idle":"2022-07-19T13:48:29.877396Z","shell.execute_reply.started":"2022-07-19T13:48:29.802537Z","shell.execute_reply":"2022-07-19T13:48:29.876049Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Lets few preparation steps to get it into the correct format for Keras \nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\nprint(X_train.shape)\nprint(lb.classes_)\n#print(y_train[0:10])\n#print(y_test[0:10])\n\n# Pickel the lb object for future use \nfilename = 'labels'\noutfile = open(filename,'wb')\npickle.dump(lb,outfile)\noutfile.close()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:48:29.883816Z","iopub.execute_input":"2022-07-19T13:48:29.884723Z","iopub.status.idle":"2022-07-19T13:48:29.916706Z","shell.execute_reply.started":"2022-07-19T13:48:29.884671Z","shell.execute_reply":"2022-07-19T13:48:29.915125Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-19T13:48:29.918412Z","iopub.execute_input":"2022-07-19T13:48:29.919508Z","iopub.status.idle":"2022-07-19T13:48:29.928541Z","shell.execute_reply.started":"2022-07-19T13:48:29.919458Z","shell.execute_reply":"2022-07-19T13:48:29.927165Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import optimizers\n\n# New model\nmodel = Sequential()\nmodel.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\nmodel.add(Activation('relu'))\n# model.add(Conv1D(256, 8, padding='same'))\nmodel.add(BatchNormalization())\n# model.add(Activation('relu'))\n# model.add(Dropout(0.25))\n# model.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\n# model.add(Conv1D(128, 8, padding='same'))\n# model.add(Activation('relu'))\n# model.add(Conv1D(128, 8, padding='same'))\n# model.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\n# model.add(Conv1D(64, 8, padding='same'))\n# model.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(12)) # Target class number\nmodel.add(Activation('softmax'))\n# opt = keras.optimizers.SGD(learning_rate=0.01, decay=5e-6, momentum=0.9, nesterov=True)\nopt = keras.optimizers.Adam(learning_rate=0.0003)\n# opt = keras.optimizers.RMSprop(learning_rate=0.00001, decay=1e-6)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\nmodel_history=model.fit(X_train, y_train, batch_size=21, epochs=500, validation_data=(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T15:18:49.768249Z","iopub.status.idle":"2022-07-19T15:18:49.769791Z","shell.execute_reply.started":"2022-07-19T15:18:49.769490Z","shell.execute_reply":"2022-07-19T15:18:49.769519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model and weights\nmodel_name = 'Emotion_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Save model and weights at %s ' % model_path)\n\n# Save the model to disk\nmodel_json = model.to_json()\nwith open(\"model_json.json\", \"w\") as json_file:\n    json_file.write(model_json)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T15:18:49.771045Z","iopub.status.idle":"2022-07-19T15:18:49.771439Z","shell.execute_reply.started":"2022-07-19T15:18:49.771255Z","shell.execute_reply":"2022-07-19T15:18:49.771273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.RMSprop(learning_rate=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","metadata":{"execution":{"iopub.status.busy":"2022-07-19T15:18:49.773307Z","iopub.status.idle":"2022-07-19T15:18:49.774013Z","shell.execute_reply.started":"2022-07-19T15:18:49.773768Z","shell.execute_reply":"2022-07-19T15:18:49.773789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = loaded_model.predict(X_test, \n                         batch_size=16, \n                         verbose=1)\n\npreds=preds.argmax(axis=1)\npreds","metadata":{"execution":{"iopub.status.busy":"2022-07-19T15:18:49.775197Z","iopub.status.idle":"2022-07-19T15:18:49.775942Z","shell.execute_reply.started":"2022-07-19T15:18:49.775722Z","shell.execute_reply":"2022-07-19T15:18:49.775747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions \npreds = preds.astype(int).flatten()\npreds = (lb.inverse_transform((preds)))\npreds = pd.DataFrame({'predictedvalues': preds})\n\n# Actual labels\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactual = (lb.inverse_transform((actual)))\nactual = pd.DataFrame({'actualvalues': actual})\n\n# Lets combined both of them into a single dataframe\nfinaldf = actual.join(preds)\nfinaldf[170:180]","metadata":{"execution":{"iopub.status.busy":"2022-07-19T15:18:49.777419Z","iopub.status.idle":"2022-07-19T15:18:49.777804Z","shell.execute_reply.started":"2022-07-19T15:18:49.777615Z","shell.execute_reply":"2022-07-19T15:18:49.777633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write out the predictions to disk\nfinaldf.to_csv('Predictions.csv', index=False)\nfinaldf.groupby('predictedvalues').count()","metadata":{"execution":{"iopub.status.busy":"2022-07-19T15:18:49.779483Z","iopub.status.idle":"2022-07-19T15:18:49.779924Z","shell.execute_reply.started":"2022-07-19T15:18:49.779713Z","shell.execute_reply":"2022-07-19T15:18:49.779731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the confusion matrix heat map plot\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n        \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Gender recode function\ndef gender(row):\n    if row == 'female_disgust' or 'female_fear' or 'female_happy' or 'female_sad' or 'female_surprise' or 'female_neutral':\n        return 'female'\n    elif row == 'male_angry' or 'male_fear' or 'male_happy' or 'male_sad' or 'male_surprise' or 'male_neutral' or 'male_disgust':\n        return 'male'\n","metadata":{"execution":{"iopub.status.busy":"2022-07-19T15:18:49.781183Z","iopub.status.idle":"2022-07-19T15:18:49.781549Z","shell.execute_reply.started":"2022-07-19T15:18:49.781366Z","shell.execute_reply":"2022-07-19T15:18:49.781383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the predictions file \nfinaldf = pd.read_csv(\"Predictions.csv\")\nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \n\n# Confusion matrix \nc = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\nprint(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","metadata":{"execution":{"iopub.status.busy":"2022-07-19T15:18:49.784561Z","iopub.status.idle":"2022-07-19T15:18:49.785067Z","shell.execute_reply.started":"2022-07-19T15:18:49.784820Z","shell.execute_reply":"2022-07-19T15:18:49.784839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification report \nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))","metadata":{"execution":{"iopub.status.busy":"2022-07-19T15:18:49.786441Z","iopub.status.idle":"2022-07-19T15:18:49.786836Z","shell.execute_reply.started":"2022-07-19T15:18:49.786645Z","shell.execute_reply":"2022-07-19T15:18:49.786669Z"},"trusted":true},"execution_count":null,"outputs":[]}]}